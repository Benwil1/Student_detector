Actually, when we look at the data for neural network training, it is kind of interesting how much the architecture really matters. You might think that more layers always equals better results, but that is not always the case. In fact, many researchers have found that simpler models can sometimes outperform more complex ones if the dataset is small. This is why we need to be careful when designing our experiments. We should always focus on the objective and make sure our tools are aligned with the problem we are trying to solve. Honestly, the field is moving so fast that it is hard to keep up with every single new development, but that is what makes it so exciting.
